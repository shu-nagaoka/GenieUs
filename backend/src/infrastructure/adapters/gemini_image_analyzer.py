"""Geminiç”»åƒåˆ†æã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

ADKéä¾å­˜ã®å®Ÿè£…ã‚¯ãƒ©ã‚¹ - Vertex AI SDKä½¿ç”¨
"""

import base64
import io
import logging
import os
from datetime import datetime
from typing import Any

import PIL.Image
import vertexai
from vertexai.generative_models import GenerativeModel, Part

from src.application.interface.protocols.image_analyzer import ImageAnalyzerProtocol


class GeminiImageAnalyzer(ImageAnalyzerProtocol):
    """Vertex AI Geminiã‚’ä½¿ç”¨ã—ãŸç”»åƒåˆ†æã®å…·ä½“çš„å®Ÿè£…"""

    def __init__(self, logger: logging.Logger, model_name: str = "gemini-2.5-flash") -> None:
        self.model_name = model_name
        self.logger = logger  # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å¿…é ˆæ³¨å…¥

        # Vertex AIåˆæœŸåŒ–
        project_id = os.getenv("GOOGLE_CLOUD_PROJECT", "blog-sample-381923")
        location = os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")

        self.logger.info(f"Vertex AIåˆæœŸåŒ–: project={project_id}, location={location}")
        vertexai.init(project=project_id, location=location)

        self.model = GenerativeModel(model_name)
        self.logger.info(f"Vertex AI GeminiåˆæœŸåŒ–å®Œäº†: model={model_name}")

    async def analyze_image_with_prompt(
        self,
        image_path: str,
        prompt: str,
        model_options: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§AIåˆ†æã‚’å®Ÿè¡Œï¼ˆç´”ç²‹ãªæŠ€è¡“å®Ÿè£…ï¼‰"""
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¿½åŠ 
            self.logger.info(f"ğŸ¤– Gemini APIå‘¼ã³å‡ºã—é–‹å§‹")
            self.logger.info(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {len(prompt)}")
            self.logger.info(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…å®¹ã‚µãƒ³ãƒ—ãƒ«: {prompt[:200]}...")

            # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å ´åˆï¼ˆç©ºã®ç”»åƒãƒ‘ã‚¹ï¼‰
            if not image_path or image_path.strip() == "":
                self.logger.info("ğŸ’¬ ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®ç”Ÿæˆã¨ã—ã¦å‡¦ç†")

                # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®é©ç”¨
                generation_config = {}
                if model_options:
                    generation_config.update(model_options)

                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆï¼ˆç”»åƒãªã—ï¼‰
                response = await self.model.generate_content_async(
                    prompt, generation_config=generation_config if generation_config else None
                )

                result = {
                    "raw_response": response.text,
                    "model_name": self.model_name,
                    "success": True,
                    "timestamp": self._get_current_timestamp(),
                    "generation_type": "text_only",
                }

                self.logger.info(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: é•·ã•={len(response.text)}æ–‡å­—")
                self.logger.info(f"ğŸ“„ ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆã‚µãƒ³ãƒ—ãƒ«): {response.text[:150]}...")
                return result

            # ç”»åƒãŒã‚ã‚‹ã‚±ãƒ¼ã‚¹ã®å‡¦ç†
            self.logger.info(f"ğŸ–¼ï¸ ç”»åƒã‚ã‚Šç”Ÿæˆ: ç”»åƒãƒ‘ã‚¹é•·={len(image_path)}")

            # ç”»åƒèª­ã¿è¾¼ã¿ï¼ˆBase64ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
            if image_path.startswith("data:image/"):
                # Base64ãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                self.logger.info("Base64ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†é–‹å§‹")
                header, data = image_path.split(",", 1)
                self.logger.info(f"Base64ãƒ˜ãƒƒãƒ€ãƒ¼: {header}")
                self.logger.info(f"Base64ãƒ‡ãƒ¼ã‚¿é•·: {len(data)}")
                image_data = base64.b64decode(data)
                image = PIL.Image.open(io.BytesIO(image_data))
                self.logger.info(f"Base64ç”»åƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”»åƒã‚’èª­ã¿è¾¼ã¿æˆåŠŸ: ã‚µã‚¤ã‚º={image.size}")
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®å ´åˆ
                self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¨ã—ã¦å‡¦ç†: {image_path}")
                image = PIL.Image.open(image_path)
                self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {image_path} ã‹ã‚‰ç”»åƒã‚’èª­ã¿è¾¼ã¿æˆåŠŸ")

            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®é©ç”¨
            generation_config = {}
            if model_options:
                generation_config.update(model_options)

            # Vertex AI Gemini APIã‚³ãƒ¼ãƒ«ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ãã®ã¾ã¾ä½¿ç”¨ï¼‰
            # ç”»åƒã‚’Vertex AI Partå½¢å¼ã«å¤‰æ›
            image_bytes = io.BytesIO()
            image.save(image_bytes, format=image.format or "PNG")
            image_bytes = image_bytes.getvalue()

            # MIMEã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
            image_format = (image.format or "PNG").lower()
            if image_format == "jpeg":
                mime_type = "image/jpeg"
            elif image_format == "png":
                mime_type = "image/png"
            elif image_format == "webp":
                mime_type = "image/webp"
            else:
                mime_type = "image/png"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            image_part = Part.from_data(mime_type=mime_type, data=image_bytes)

            # éåŒæœŸã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
            response = await self.model.generate_content_async(
                [prompt, image_part], generation_config=generation_config if generation_config else None
            )

            # ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ï¼ˆæœ€å°é™ã®æŠ€è¡“çš„å‡¦ç†ã®ã¿ï¼‰
            result = {
                "raw_response": response.text,
                "model_name": self.model_name,
                "success": True,
                "timestamp": self._get_current_timestamp(),
                "generation_type": "image_with_prompt",
            }

            self.logger.info("Gemini API call completed")
            return result

        except Exception as e:  # noqa: BLE001
            self.logger.error(f"âŒ Gemini API error: {e}")
            return {
                "raw_response": "",
                "model_name": self.model_name,
                "success": False,
                "error": str(e),
                "timestamp": self._get_current_timestamp(),
            }

    def _get_current_timestamp(self) -> str:
        """ç¾åœ¨ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—"""
        return datetime.now().isoformat()
